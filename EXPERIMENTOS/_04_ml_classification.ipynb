{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIRTUAL PATIENT -  DATA MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUTS**:\n",
    "* *data/03_virtual_patients_db.xlsx*\n",
    "* *data/02_112_patients_db.xlsx*\n",
    "* *data/03_virtual_patients_cie_692,76.xlsx*\n",
    "* *data/04_casos_específicos.xlsx*\n",
    "\n",
    "**OUTPUTS**:\n",
    "* *data/04_virtual_patients_db_lemmatized.xlsx*\n",
    "* *data/04_virtual_patients_and_112_db_lemmatized.xlsx*\n",
    "* *data/models/...*\n",
    "\n",
    "**NOTAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # instead of CountVectorizer and TfidfTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET SEED TO MAKE EXPERIMENTS CONSISTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET DATA AND PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF lemmatized data has been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_patients = pd.read_excel(\"data/04_virtual_patients_db_lemm.xlsx\")\n",
    "virtual_patients.fillna('', inplace=True)\n",
    "\n",
    "db_112 = pd.read_excel(\"data/04_112_patients_db_lemm.xlsx\")\n",
    "db_112.fillna('', inplace=True)\n",
    "\n",
    "one_cie = pd.read_excel(\"data/03_virtual_patients_cie_692,76.xlsx\")\n",
    "one_cie.fillna('', inplace=True)\n",
    "\n",
    "space_cases = pd.read_excel(\"data/04_casos_específicos.xlsx\")\n",
    "space_cases.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create merged database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_112[\"F_TEXT\"] = db_112[\"T_L_TEXT\"]\n",
    "virtual_patients[\"F_TEXT\"] = virtual_patients[\"L_TEXT\"]\n",
    "\n",
    "virtual_patients_and_db_112 = virtual_patients\n",
    "virtual_patients_and_db_112 = virtual_patients_and_db_112.append(db_112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF lemmatized data has NOT been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_patients = pd.read_excel(\"data/03_virtual_patients_db.xlsx\")\n",
    "db_112 = pd.read_excel(\"data/02_112_patients_db.xlsx\")\n",
    "\n",
    "virtual_patients.fillna('', inplace=True)\n",
    "db_112.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to lower case + Tokenization + Remove Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(PREVIOUSLY DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "count=0\n",
    "def lemmatize(text):\n",
    "    global count\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(w, get_pos_tag(w)) for w in nltk.word_tokenize(text)])\n",
    "\n",
    "def get_results(y_test,y_pred):\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize individually and merge Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_patients['L_TEXT'] = virtual_patients['TEXT'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_112['L_TEXT'] = db_112['TEXT'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_patients_and_db_112 = virtual_patients\n",
    "virtual_patients_and_db_112.append(db_112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_patients.to_excel(\"./data/04_virtual_patients_db_lemm.xlsx\", index=False) #[['CIE','AGE','SEX','RISK','L_TEXT']]\n",
    "db_112.to_excel(\"./data/04_112_patients_db_lemm.xlsx\", index=False)[['CIE','AGE','SEX','RISK','L_TEXT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# EXPERIMENT __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIVIDE DATA IN TRAIN AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    virtual_patients_and_db_112.F_TEXT, \n",
    "    virtual_patients_and_db_112.RISK, \n",
    "    test_size=0.25,\n",
    "    random_state=76\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL AND TESTING - MULTINOMIAL (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline: TF-IDF Matrix, select n elements and define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=5, max_df=0.7)),\n",
    "                     ('selec',  SelectKBest(chi2, k='all')),\n",
    "                     ('model', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'model__alpha': (1e-2, 1e-3)}\n",
    "param_pipeline = GridSearchCV(pipeline, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = param_pipeline.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL AND TESTING - LINEAR SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=5, max_df=0.7)),\n",
    "                     ('selec',  SelectKBest(chi2, k='all')),\n",
    "                     ('model', LinearSVC())])\n",
    "\n",
    "model2 = pipeline.fit(X_train, y_train)\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL AND TESTING - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=5, max_df=0.7)),\n",
    "                     ('selec',  SelectKBest(chi2, k='all')),\n",
    "                     ('model', RandomForestClassifier(n_estimators=100))])\n",
    "\n",
    "model3 = pipeline.fit(X_train, y_train)\n",
    "y_pred = model3.predict(X_test)\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL AND TESTING - PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=5, max_df=0.7)),\n",
    "                     ('selec',  SelectKBest(chi2, k='all')),\n",
    "                     ('model', MLPClassifier(solver='lbfgs'))])\n",
    "\n",
    "model4 = pipeline.fit(X_train, y_train)\n",
    "y_pred = model4.predict(X_test)\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL AND TESTING - ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('nb', model1), ('ls', model2), ('rf', model3), ('pe', model4)],\n",
    "    voting='hard')\n",
    "\n",
    "ensemble = ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "get_results(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "## STORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model1, open('web/static/models/model1.pkl', 'wb'))\n",
    "pickle.dump(model2, open('web/static/models/model2.pkl', 'wb'))\n",
    "pickle.dump(model3, open('web/static/models/model3.pkl', 'wb'))\n",
    "pickle.dump(model4, open('web/static/models/model4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pickle.load(open('web/static/models/model1.pkl', 'rb'))\n",
    "model2 = pickle.load(open('web/static/models/model2.pkl', 'rb'))\n",
    "model3 = pickle.load(open('web/static/models/model3.pkl', 'rb'))\n",
    "model4 = pickle.load(open('web/static/models/model4.pkl', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
